
Making the best guess in hangman
================================
Strategy 1. Highest frequency
Pick the letter that is most frequently occurring in the list of possible words. 
Problem: imagine if we lived in a world where every word ended in "-ing". 
Then chosing i, n, or g often times doesn't make any distinction between the possible words.

Strategy 2. 2-Partition
Pick the letter that most evenly splits the list of possible words in half.
That way on average, we reduce the list in half with each guess.

Strategy 3. N-Partition
Picking a letter partitions the list of possible words into more 
than just has the letter or doesn't. Consider guessing the 
letter 'O' for a four letter word. Consider the list of words 
being only STOP, WOLF, FOOT, and RACE.
Further suppose this is the first guess of the hangman game. 
This would generate the follow four situations.
1. __O_ with the possible secret word STOP
2. _O__ with the possible secret word WOLF
3. _OO_ with the possible secret word FOOT
4. ____ with the possible secret word RACE

Pick the letter that partitions the possible words with proportions that 
have the highest Shannon entropy. Also, compare the entropy with just 
guessing a word at random from the list of possible words. Pick the 
letter or random word guess with the highest entropy.

Entropy = sum - Pi * Log2(1/Pi)  where Pi is the porportion of partition i to the
           i                   size of the whole unpartitioned list.

Strategy 4. N-Partition with R look ahead
Strategy 3 is a greedy algorithm. Perhaps something like the ID3 algorithm that 
selects the single feature with the best score at each iteration. It could get 
stuck in a local maxima. 

Consider following four letter guesses along with their partitioning of the possible words:
1. Guess A splits the words into equal thirds 
   entropy(A) = 3 * - 1/3 -Log2(1/3) = 1.58596
2. Guess B splits the words into equal thirds, same exact partition as
   guess A
3. Guess C. splits the words into equal halves
   entropy(C) = 2 * - 1/2 -Log2(1/2) = 1
4. Guess D. splits the words into equal halves. This partition further splits
   each partition to halves again. 
   entropy(C) = 2 * - 1/2 -Log2(1/2) = 1

So, if we only look one move ahead, picking A or B would have the 
highest entropy. However, if we think two moves ahead, we see the 
following entropies:
Guess(A then B) = Guess(B then A) = 1.58596

Guess(C then D) = Guess(D then C) = 2

So, guessing either C or D would allow use the most information gain.
We only see this when looking two moves ahead.

But we don't submit multiple letter guesses at time. So we select one
of C or D, and completely re-evaluate with the new information revealed
after making the single letter guess.

This reminds me of a quote:
  "In preparing for battle I have always found that plans are useless, 
   but planning is indispensable.
   - Dwight D. Eisenhower, general and president (1890 - 1969)


Experimental results
--------------------
When testing we found that looking ahead didn't improve the strategy.
This could be property of the problem space we are considering.

Find the the global maximum is like trying to find the top of a mountain
in the dark. With one move look ahead, we just take a step in the steepest
direction. With multiple look ahead, it is as though we have a weak flashlight
and we can see a few steps ahead. We just select the steepest looking set
of steps as seen from the flashlight.

The goal is to get to the top of the mountain in the fewest number of steps.
It could be that the taking the single steepest step is just as good as
using the flashlight. I suspect that is the case with the game of hangman.

Consider entropy levels with each guess of the secret word UNDERCOOLING
Entropy after guess 1 = 5.566 
Entropy after guess 1 = 4.621 
Entropy after guess 1 = 3.496 
Entropy after guess 1 = 1.880 

In four guesses we got the word. The entropy, or the amount of information
gained, with the earlier guesses is greater than in later guesses. It is as
if the mountain is a very rounded hill that is steepest around the base of
the hill. The hill is relatively flat towards the top. This help confirm the
intuition that the hangman entropy mountain doesn't suffer from many deceptive 
local maxima. So, looking ahead further than one move doesn't help much.



